{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles y bosques aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librería para arboles**\n",
    ">from sklearn.tree import DesicionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la capacidad de elegir en que cajita va cada elemento, el árbol de desición es un algoritmo supervisado, cuando la variable objetivo a ser clasificada es discreta o categórica ya sea de un solo tipo o de varios tipos a escoger, las predictoras se pueden volver númericas o categóricas, puede ser complicado establecer un conjunto de reglas para ir recorriendo el árbol, desde la raiz hasta los nodos hojas para una variable objetivo para una variable discreta o categórica, esto terminá siendo un conjunto de regas if else que se representan como divisiones, el árbol de decisión se puede ocupar cuando la decisión final esta basada en una serie de pasos escalonados o en pautas que cada vez las variables pueden tomar un valor o otro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pierde mucha información en variables númericas por que no queda catalogado por ninguna regla de desición, si se usan categorías tienden a ser más seguros los grupos de desición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se pueden crear categorías apartir de los números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No importa demasiado que la limpieza de datos por que al final se terminan clasificando nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## las matemáticas de un árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toma una variable e intenta clasificar dependiendo de una serie de valores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneidad en los datos\n",
    "Se refiere a que el nodo que separa los datos de otros datos estos datos nuevos agrupados seán lo mayor homegeneos posibles o que no se distinga diferencia entre ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identificar variables que sean lo mas homogeneas posibles en base a algoritmos\n",
    ">** La entropia\n",
    ">\n",
    ">** La ganancia o la perdida de información\n",
    ">\n",
    ">** id3\n",
    ">\n",
    ">** la varianza\n",
    ">\n",
    ">** la moda del arból"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La entropía\n",
    "Utiliza la teoría de información entre más puros son los nodos de un árbol, se requiere menos información para representarlos\n",
    "**Por ejemplo:** Para encriptar una señal, si los elementos que constituyen el mensaje son muy homogeneos en general se consumiran menos bits para llevar acabo la transmisión, la configuración que requiere menos información es la preferida.\n",
    "* **Teoría de la información**\n",
    "Imaginemos que tenemos un mensaje que solo tiene la letra \"a\" y tenemos que codificar un mensaje con 10 \"a\", en vez de enviar las 10 \"a\" si para cada letra necesitariamos los 8 bits, sustituimos por un símbolo, podremos reducir el tamaño de la información, si tenemos 5 \"a\" y 5 \"b\" en vez de enviar su codificación en ascii solo enviariamos el símbolo que corresponde a la letra \"a\" y a la letra \"b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La impunidad o heterogeneidad\n",
    "Se puede representar de un nodo se puede representar con la entropía, la interpretación de la entropía, la teoría de la información es el número mínimo de bits que hacen falta para codificar una determinidada clasificación de un miembro arbitrario de un conjunto fijado a priori.\n",
    "Cualquier cambio o reducción de una entropia de un nodo se medira como una ganancia.\n",
    "En el arból si añadimos un nodo y este resulta en mayor ganancia de información este se añade a la configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuanto el algoritmo de entropía nos marca, una mayor ganancia, es esta ganancia la que se utiliza como nodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos para crear arboles de desición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ramas consideradas anteriormente pueden seguir siendo consideradas para el siguiente nodo de ramificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo ID3\n",
    "1. Calculamos la entropia inicial basandonos en la **variable objetivo** a predecir \n",
    "2. Calculamos la ganancia de la información para cada variable candidata para un nodo,\n",
    "Seleccionamos la variable que nos da la máxima, ganancia de información como nodo de desición\n",
    "3. Repetimos el paso dos para cada rama (valor) de cada nodo(variable candidata). El núevo nodo identificado es un nodo hoja\n",
    "4. Comprobamos si el nodo hoja clasifica correctamente todos los datos. Si es así pasaremos con esa rama. si no es así regresamo al paso dos para ramificarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otros algoritmos\n",
    "* índice Gini: Si la variable a predecir es binaria\n",
    "* Detector Automático de la interacción con Chi Cuadrado(CHAID): Encontrar las diferencias entre un nodo padre y un sunodo hijo, puede gestionar más de dos categorías\n",
    "* Reducción de la varianza: Gestionar el problema de una variable numérica continua como una variable objetivo en un árbol de desición\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La poda de árbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un árbol puede seguir creciendo hasta el infinito creando un problema de oversiting, para evitar que un árbol tenga tantos nodos como muestras del dataset, cuando el árbol aprende de los datos pero los clasifica terriblemente mal cuando se presentan otros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces es mejor tener un árbol más pequeño pero que puede clasicar diferentes datasets o información y no solo una información especifica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reducción del error en la poda\n",
    "* Poda del coste de complejidad\n",
    "1. Definir la taza de error para un árbol T en un dataset D err(T,D)\n",
    "2. El número de nodos terminales en el árbol viene dado por leaves \n",
    "3. Definimos la función M como el coeficiente: \n",
    "4. Calculamos M para todos los subarboles de T\n",
    "5. El subárbol que minimiza M se elige para ser eliminado del árbol original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va reduciendo el coste de complejidad, se va reduciendo el número de pasos para llegar a la clasificación pero se va incrementando el error, nosotros debemos ponernos un número para determinar un N máximo para decidir hasta donde el corte, queremos podar el árbol del cual habiamos partido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Las variables númericas contínuas o NAs\n",
    "Qué pasa con los valores NAs o nulos \n",
    "* Variables contínuas \n",
    "En el caso de tener variables contínuas hay que encontrar umbrales óptimos de corte para convertirlos a categorías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * Como definimos ese umbral\n",
    "* * 1. Ordenamos los datos de forma ascendente\n",
    "* * 2. Marcamos los rangos de transición de una categoría a otra de la predicada\n",
    "* * 3. Calculamos el punto medio de cada umbral de cambio: C1, C2, C3\n",
    "* * 4. Las categorías serán marcadas por: T<C1, C1<T<C2, C2<T<C3, T>C3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores desaparecidos en el dataset \n",
    "Una de las formas de resolver el problema es asignar el valor más común, de la categoría donde falta una variable o si ya tenemos la predicción echa utilizar el valor que ha predicho en otros casos esa información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de regresión \n",
    "1. Empezamos con un nodo con todas las observaciones \n",
    "* Calculamos la media y la varianza de la variable objetivo\n",
    "2. Calculamos la reducción de la varianza para todos los posibles candidatos a variable del siguiente nodo eligiendo la que da máxima reducción de la misma en el nodo\n",
    "3. Para cada nodo hoja comprobamos: \n",
    "* La reducción máxima de la varianza es menor que un valor fijo \n",
    "* El número de observaciones en dicho nodo es menor que un valor fijo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algóritmos por unión, consiste en una combinación de módelos similares o independientes.\n",
    "La unión significara un módelo mejor que un módelo independiente, utilizaremos el promedio para saber que algóritmo es mejor para el ensamblaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmos de ensamblaje\n",
    "* Métodos de promedio: Se crean varios modelos similares e independientes y se hace un promedio de las predicciones de cada módelo.\n",
    "* Métodos de impulso: Reducir el sesgo del estimador combinado construyéndolo de forma secueción de los estimadores base. Así conseguimos un módelo más robusto a partir de varios más débiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tomamos una muestra aleatoria simple de tamaño n con remplazo\n",
    "2. Tomamos una muestra aleatoria simple de variables predictoras sin remplazo\n",
    "3. Construimos un árbol de regresión con los predictores elegidos en 2 y sin podar el árbol\n",
    "4. Clasificamos las observaciones fuera de la bolsa con dicho árbol y almacenamos el valor o de la clase asignada para cada una.\n",
    "5. Repetimos los pasos 1 a 4 un número largo de veces para tener el bosque de árboles\n",
    "6. La predicción final es el promedio de las observaciones de todos los árboles o para clasificación la clase con mayoría de votos en el conjunto de árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No necesitan poda, las bolsas que quedan fuera veremos que tan bien clasifica, tantas ramas queremos será el número de iteraciones que haremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase que más se haya predecido será la predicción final para dicha observación, cual es la clase en la que están más de acuerdo los árboles que hemos construido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para que funcionan los randoms forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se van tomando diferentes módelos y se va minimizando el error al avanzar en la producción de árboles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros importantes\n",
    "* Tamaño del nodo(min_samples_leaf) una hoja puede tener el mínimo número de muestras para ser considerada como tal y ver si el error aumenta o se reduce\n",
    "* Número de árboles(n_estimators) númerod de árboles a crear, por lo regular debe ser un número grande, depende del número de columnas que tenga el dataset\n",
    "* Número de predictores muestrados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen\n",
    "* Los árboles de desición son un algoritmo de clasificación que utiliza variables predictoras continuas o categóricas para predecir una categória\n",
    "* Un nodo se divide en subnodos para obtener distribuciones homogeneas con técnicas como la ganancia de información, el indice Gini o la reducción de la varianza\n",
    "* Los árboles de regresión son similares a los de decisión pero en este caso la variable de decisión es contínua\n",
    "* Los bosques aleatorios son algóritmos de aprendizaje por combinación donde combinamos diversos árboles para la decisión final por mayoría o por promedio. Son mas eficientes que una sola decisión ya que maximizan la reducción de la varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e310ee305116be7a1f6c3fc36efd30b44e9b58a3d111aad7f613c214e7b9dffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
